1. What is Simple Linear Regression?

    Simple Linear Regression is a statistical method that models the relationship between two variables: one independent variable (X) and one dependent variable     (Y). It fits a straight line to the data using the equation:
    Y=mX+c
    where:
    Y = predicted value,
    m = slope (coefficient),
    X = independent variable,
    c = intercept.

2. What are the key assumptions of Simple Linear Regression?

    Linearity – There is a linear relationship between X and Y.
    Independence – Observations are independent.
    Homoscedasticity – Constant variance of residuals.
    Normality of Residuals – The residuals are normally distributed.
    No multicollinearity – Only one independent variable (so not relevant here, but important in multiple regression).

3. What does the coefficient m represent in the equation Y=mX+c?

    It represents the slope of the regression line. For every one-unit increase in X, Y changes by m units. 
    It shows the strength and direction of the relationship.

4. What does the intercept c represent in the equation Y=mX+c?

    The intercept is the value of Y when X is zero. It gives a starting point for the prediction line.

5. How do we calculate the slope m in Simple Linear Regression?

    Using the formula: m = [ n * Σ(XY) - (ΣX)(ΣY) ] / [ n * Σ(X²) - (ΣX)² ]
    It minimizes the difference between predicted and actual Y values (least squares method).

6. What is the purpose of the least squares method in Simple Linear Regression?

    To find the best-fitting line by minimizing the sum of the squared differences (residuals) between the observed values and predicted values.

7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?

    R² measures the proportion of variance in the dependent variable that is predictable from the independent variable.
    
    R² = 0 → No relationship
    
    R² = 1 → Perfect fit
    Example: R² = 0.8 means 80% of the variation in Y is explained by X.

8. What is Multiple Linear Regression?

    An extension of SLR that models the relationship between two or more independent variables and a dependent variable.
    Formula:
    Y=b_0 + b_1 * X_1 + b_2 * X_2 +...+ b_n * X_n

9. What is the main difference between Simple and Multiple Linear Regression?

    SLR: Only 1 independent variable
    MLR: 2 or more independent variables

10. What are the key assumptions of Multiple Linear Regression?

    Same as SLR, plus:
    No multicollinearity: Independent variables should not be highly correlated.
    No autocorrelation: Especially in time-series data.

11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model.

    Heteroscedasticity occurs when the variance of residuals is not constant.
    It violates regression assumptions.
    Leads to inefficient and biased estimates, incorrect p-values.

12. How can you improve a Multiple Linear Regression model with high multicollinearity?

    Remove or combine highly correlated predictors.
    Use Principal Component Analysis (PCA).
    Apply Ridge or Lasso regularization.

13. What are some common techniques for transforming categorical variables for use in regression models?

    1. One-Hot Encoding (creates binary columns)
    2. Label Encoding (assigns numerical labels)
    3. Ordinal Encoding (for ordered categories)

14.  What is the role of interaction terms in Multiple Linear Regression?

    Interaction terms (e.g., X_1 * X_2)
    allow the effect of one independent variable to depend on the level of another. They capture combined effects.

15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?

    Interpretation of intercept in SLR vs. MLR:
    In SLR: Value of Y when X = 0
    In MLR: Value of Y when all Xs = 0, which may not be realistic.

16. What is the significance of the slope in regression analysis, and how does it affect predictions?

    Significance of slope in regression:
    The slope shows how much the dependent variable is expected to change when the independent variable changes by one unit, holding other variables constant.

17. How does the intercept in a regression model provide context for the relationship between variables?

    It gives a baseline value of the outcome when all inputs are zero. Helps understand model behavior at the starting point.

18. What are the limitations of using R² as a sole measure of model performance?

    Doesn’t indicate if predictors are meaningful.
    Can increase with more variables, even if they are irrelevant.
    Doesn’t measure overfitting.

19. How would you interpret a large standard error for a regression coefficient.

    It indicates the estimate is unstable or imprecise. It might not be statistically significant.

20. How can heteroscedasticity be identified in residual plots, and why is it important to address it.

    Detected via residual plots or Breusch-Pagan test.
    Important because it affects standard errors, making hypothesis tests unreliable.

21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R².

    High R² but low adjusted R² means?
    Model might be overfitting – added variables do not significantly improve prediction.

22. Why is it important to scale variables in Multiple Linear Regression?

    Scaling ensures:
    Equal treatment of features
    Faster convergence in gradient-based optimization
    Avoids dominance by larger-scale variables

23. What is polynomial regression.

    A form of regression where the relationship between X and Y is modeled as an nth-degree polynomial.

24. How does polynomial regression differ from linear regression.

    Linear regression: Straight-line fit
    Polynomial regression: Curved fit (e.g., quadratic, cubic)

25. When is polynomial regression used.

    When data shows non-linear patterns.
    To fit complex curves with a better approximation.

26. What is the general equation for polynomial regression.

    General equation of Polynomial Regression: b_0 + b_1*X + b_2*X^2 +...+b_n+X^n

27. Can polynomial regression be applied to multiple variables.

    Yes, it's called Multivariate Polynomial Regression, combining multiple inputs and powers of variables.

28. What are the limitations of polynomial regression?

     Limitations of Polynomial Regression:
    
    Risk of overfitting, especially with high degrees
    Poor extrapolation
    Sensitive to outliers

29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?

    Methods to evaluate polynomial model fit:

    R² and Adjusted R²
    Mean Squared Error (MSE)
    Cross-validation
    Visual inspection of residuals

30. Why is visualization important in polynomial regression.

   . Helps see how well the model fits the curve
   . Detects underfitting/overfitting
   . Reveals anomalies or outliers

31. How is polynomial regression implemented in Python?

    Using PolynomialFeatures and LinearRegression from sklearn:

    from sklearn.preprocessing import PolynomialFeatures
    from sklearn.linear_model import LinearRegression
    from sklearn.pipeline import make_pipeline
    
    model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())
    model.fit(X, y)
    predictions = model.predict(X)
